# üì¶ SmartWaste API

**Sistema de gesti√≥n inteligente de residuos urbanos**  
Este backend basado en FastAPI permite gestionar usuarios, sensores, contenedores, rutas y lecturas para optimizar la recolecci√≥n de residuos urbanos mediante IoT.

---

## üöÄ Clonaci√≥n y Ejecuci√≥n del Proyecto

### üîß Requisitos previos

- Python 3.10 o superior  
- MySQL instalado y corriendo (o Docker para usar el contenedor)  
- Git  
- pip/venv o [Poetry](https://python-poetry.org/) (recomendado)

---

### üì• Clonar el repositorio

```bash
git clone https://github.com/JoseLuisCM663/SmartWasteApi.git
cd SmartWasteApi
```

---

### üêç Configurar entorno virtual (opcional pero recomendado)

**Con `venv`:**

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

---

### üì¶ Instalar dependencias

**Con pip:**

```bash
pip install -r requirements.txt
```

---

### ‚öôÔ∏è Configuraci√≥n del entorno

Copiar archivo `.env.example` a `.env`:

```bash
cp .env.example .env
```

Modificar las variables en el archivo `.env`:

```env
DB_USERNAME=root
DB_PASSWORD=12345678
DB_HOST=localhost
DB_PORT=3306
DB_NAME=smartwaste
SECRET_KEY=ejemplo
ALGORITHM=HS256
```

---

### üõ¢Ô∏è Configurar base de datos

1. Crear base de datos:

```sql
CREATE DATABASE smartwaste;
```

---

### üöÄ Ejecutar la aplicaci√≥n

```bash
uvicorn app.main:app --reload
```

La aplicaci√≥n estar√° disponible en:  
‚û°Ô∏è [http://localhost:8000](http://localhost:8000)

---

### üê≥ Ejecuci√≥n con Docker

Construir y ejecutar los contenedores:

```bash
docker-compose up --build
```

La aplicaci√≥n estar√° disponible en:  
‚û°Ô∏è [http://localhost:8000](http://localhost:8000)

---

## üìö Documentaci√≥n de la API

- Swagger UI: [http://localhost:8000/docs](http://localhost:8000/docs)  
- Redoc: [http://localhost:8000/redoc](http://localhost:8000/redoc)


---

## üõ†Ô∏è Estructura del Proyecto

```
app/
‚îú‚îÄ‚îÄ config/        # Configuraciones principales
‚îú‚îÄ‚îÄ crud/          # Operaciones CRUD para la base de datos
‚îú‚îÄ‚îÄ models/        # Modelos de SQLAlchemy
‚îú‚îÄ‚îÄ schemas/       # Esquemas Pydantic para validaci√≥n
‚îú‚îÄ‚îÄ routes/        # Endpoints para acceso a las funciones
‚îú‚îÄ‚îÄ utils/         # 
‚îú‚îÄ‚îÄ main.py         # Rutas principales
‚îî‚îÄ‚îÄ database.py    # Configuraci√≥n de la base de datos
```

---
#  Seeder de Datos

Este repositorio incluye scripts para poblar la base de datos de **SmartWasteAPI** con datos de prueba.

---

## üöÄ Funcionalidades incluidas

### 1. Seeder principal (`config/seeder.py`)

Este script inserta datos iniciales en las siguientes tablas:

- ‚úÖ **Usuarios** (Admin, Usuario, Chofer)
- ‚úÖ **Rutas** de recolecci√≥n
- ‚úÖ **Contenedores** (asociados a rutas)
- ‚úÖ **Sensores** (asociados a contenedores)
- ‚úÖ **Relaci√≥n Usuario-Ruta**

#### ‚ñ∂Ô∏è Uso

```bash
python config/seeder.py --usuarios 3 --rutas 2 --contenedores 2 --sensores 2 --seed 42
```

#### üìå Argumentos

| Par√°metro        | Descripci√≥n                               | Valor por defecto |
|------------------|--------------------------------------------|-------------------|
| `--usuarios`     | N√∫mero de usuarios a crear                 | 3                 |
| `--rutas`        | N√∫mero de rutas                            | 2                 |
| `--contenedores` | N√∫mero de contenedores                     | 2                 |
| `--sensores`     | N√∫mero de sensores                         | 2                 |
| `--seed`         | Semilla para datos aleatorios reproducibles | Opcional          |

---

### 2. Generador de Lecturas de Sensor

Genera lecturas simuladas para sensores.

#### üß™ Uso

```python
from crud.seeder import generar_lecturas_sensor

generar_lecturas_sensor(sensor_id=1, cantidad=100, valor_min=10, valor_max=90)
```

#### üîß Par√°metros

- `sensor_id`: ID del sensor a asociar las lecturas
- `cantidad`: N√∫mero de lecturas a generar
- `valor_min`: Valor m√≠nimo de lectura
- `valor_max`: Valor m√°ximo de lectura
- `intervalo_minutos`: (opcional) intervalo entre lecturas

---

### 3. Generador de Bit√°coras de Recolecci√≥n

Inserta bit√°coras con datos asociados a rutas y contenedores.

#### üßæ Uso

```python
from crud.seeder import poblar_bitacoras
from app.database import SessionLocal

db = SessionLocal()
poblar_bitacoras(db, cantidad=10)
```

---

## üóÇ Estructura de Carpetas

```
config/
  ‚îî‚îÄ‚îÄ seeder.py           # Seeder principal
crud/
  ‚îî‚îÄ‚îÄ seeder.py           # Funciones de utilidad (lecturas, bit√°coras)
```
# üßπ Preprocesamiento y Limpieza de Datos - SmartWasteApi

Este m√≥dulo forma parte del backend de **SmartWasteApi**, encargado del **ETL (Extracci√≥n, Transformaci√≥n y Carga)** de datos recolectados por sensores y bit√°coras del sistema de gesti√≥n inteligente de residuos.

## üìÇ Funcionalidades implementadas

### 1. `exportar_lecturas_sensor(sensor_id, ruta_csv, db)`

Exporta las lecturas de un sensor espec√≠fico a un archivo CSV, siguiendo los pasos del proceso ETL:

- **Extracci√≥n:** Consultas a la base de datos con SQLAlchemy por `Sensor_Id`.
- **Transformaci√≥n:** Se aplican t√©cnicas de:
  - Conversi√≥n y ordenaci√≥n temporal (`datetime`)
  - Eliminaci√≥n de duplicados
  - Filtrado y clipping de valores (0-100)
  - Resampleo a intervalos regulares (5 minutos)
  - Interpolaci√≥n lineal para valores faltantes
  - Redondeo de valores
  - Creaci√≥n de variables adicionales: `Hora`, `D√≠a de la semana`
- **Carga:** Generaci√≥n y guardado de un archivo `.csv` limpio y estructurado.

### 2. `etl_bitacoras(db)`

Realiza la limpieza y exportaci√≥n de datos de dos tablas de bit√°coras:

- **Bit√°cora de Recolecci√≥n:**
  - Se excluyen registros sin fecha o ruta v√°lida
  - Se filtran valores at√≠picos en duraci√≥n (>500)
  - Se imputan valores por defecto para campos nulos (ej. observaciones)
- **Bit√°cora de Contenedor:**
  - Se validan fechas y IDs de contenedores
  - Se eliminan valores de porcentaje de llenado fuera de rango (0-100)
  - Se etiquetan estados desconocidos

Retorna dos `DataFrames` limpios listos para an√°lisis o exportaci√≥n.

## üöÄ Endpoints disponibles

### `GET /exportar-sensor/?sensor_id=ID`

- Ejecuta `exportar_lecturas_sensor`
- Devuelve un archivo `.csv` limpio con las lecturas del sensor especificado

### `GET /exportar-bitacoras/`

- Ejecuta `etl_bitacoras`
- Devuelve un archivo `.zip` con los siguientes CSVs:
  - `bitacora_recoleccion_etl.csv`
  - `bitacora_contenedor_etl.csv`

## üìå Ubicaci√≥n del c√≥digo

- L√≥gica principal de ETL: `app/utils/etl_export.py`
- Rutas de exportaci√≥n: `app/routers/exportar.py`

## üõ† Requisitos

- Pandas
- Numpy
- SQLAlchemy
- FastAPI
- Zipfile (m√≥dulo est√°ndar de Python)

## üìÅ Ejemplo de uso

```bash
curl -X GET "http://localhost:8000/exportar-sensor/?sensor_id=1" -o lecturas_sensor_1.csv

curl -X GET "http://localhost:8000/exportar-bitacoras/" -o bitacoras_etl.zip
```

## üìà Objetivo del m√≥dulo

Garantizar datos limpios, consistentes y listos para an√°lisis de comportamiento, modelos predictivos o visualizaciones, reduciendo errores por datos at√≠picos o faltantes.

---
# Modelo de An√°lisis Supervisado (ML) - Clasificaci√≥n de Rutas

Este m√≥dulo implementa un **modelo supervisado** para clasificar rutas de recolecci√≥n como **Eficientes** o **Ineficientes**, utilizando datos hist√≥ricos de bit√°coras de recolecci√≥n y contenedores.

---

## Estructura de la API

Se exponen los siguientes endpoints en FastAPI:

### Entrenamiento del modelo

```
POST /ml/entrenar/
```

**Descripci√≥n:**
Entrena un modelo de clasificaci√≥n usando los CSV de bit√°coras de recolecci√≥n y contenedores.

**C√≥digo de ejemplo:**

```python
resultado = ml_model.entrenar_modelo_bitacoras(
    "datos/bitacora_recoleccion_etl.csv",
    "datos/bitacora_contenedor_etl.csv"
)
```

**Respuesta esperada:**

```json
{
  "mensaje": "Modelo entrenado y guardado",
  "accuracy": 0.85,
  "f1_score": 0.84,
  "reporte": "Classification report completo..."
}
```

---

### Predicci√≥n de rutas

```
GET /ml/predecir/?bitacora_id={id}
```

**Descripci√≥n:**
Predice si una ruta de recolecci√≥n (bit√°cora) es **Eficiente** o **Ineficiente**, usando los datos de BitacoraRecoleccion y BitacoraContenedor.

**C√≥digo de ejemplo:**

```python
resultado = ml_model.predecir_ruta_por_bitacora(bitacora_id, db)
```

**Respuesta esperada:**

```json
{
  "bitacora_id": 123,
  "prediccion": "Eficiente",
  "features": {
    "Tiempo_Duracion": 90,
    "Cantidad_Contenedores": 5,
    "Porcentaje_Llenado": 0.85,
    "Recolectado": 1.0
  }
}
```

---

## Proceso de Entrenamiento

1. **Carga de datos:** Se leen los archivos CSV de bit√°coras de recolecci√≥n y contenedores.
2. **Preprocesamiento:**

   * Conversi√≥n de fechas.
   * Agregaci√≥n de m√©tricas por ruta: promedio de llenado y recolectado.
3. **Definici√≥n de etiqueta (target):**

   * `Eficiente` (1): `Tiempo_Duracion <= 100` y `Recolectado >= 0.8`
   * `Ineficiente` (0): caso contrario
4. **Selecci√≥n de features:**

   * `Tiempo_Duracion`
   * `Cantidad_Contenedores`
   * `Porcentaje_Llenado`
   * `Recolectado`
5. **Split Train/Test:** 70% entrenamiento, 30% prueba.
6. **Modelo:** RandomForestClassifier con 100 estimadores.
7. **Evaluaci√≥n:**

   * Accuracy
   * F1-score
   * Classification report
8. **Guardado del modelo:** `datos/modelos/modelo_rutas.pkl`

---

## M√©tricas de Evaluaci√≥n

* **Accuracy:** Porcentaje de predicciones correctas sobre el total.
* **F1-score:** Media arm√≥nica entre precisi√≥n y recall.
* **Classification Report:** Desglose de precisi√≥n, recall y F1-score por clase.

---

## Dependencias

* `pandas`
* `numpy`
* `scikit-learn`
* `joblib`
* `fastapi`
* `sqlalchemy`

---

## Uso en FastAPI


Despu√©s de incluir el router, se pueden usar los endpoints `/ml/entrenar/` y `/ml/predecir/`.

---
# Modelo de An√°lisis No Supervisado (ML)

## Objetivo

Implementar un **modelo de an√°lisis no supervisado** sobre los datos de lecturas de sensores para identificar patrones o agrupaciones naturales. En este proyecto se utiliz√≥ **K-Means Clustering** como t√©cnica principal.

---

## Metodolog√≠a

1. **Preparaci√≥n de datos**

   * Se utiliza el CSV generado previamente con todas las lecturas de sensores (`lecturas_todos_sensores.csv`).
   * Columnas consideradas: `Valor`, `Hora`, `DiaSemana`.
   * Se transforma `DiaSemana` a valores num√©ricos (`0=Monday, 6=Sunday`).
   * Se escalan las variables usando `StandardScaler` para que todas tengan la misma importancia en el clustering.

2. **Entrenamiento del modelo**

   * Se utiliza `KMeans` de `sklearn` con n√∫mero de clusters configurable (`n_clusters`).
   * El modelo se ajusta a los datos escalados y se asigna un cluster a cada lectura.
   * El modelo entrenado se guarda en: `datos/modelos/kmeans_model.pkl`.

3. **Generaci√≥n de informe**

   * Se genera un **PDF** que incluye:

     * Silhouette Score para evaluar la calidad del clustering.
     * Gr√°ficos 2D de:

       * Valor vs Hora
       * Valor vs D√≠a de la semana
     * Tabla con los promedios de cada variable por cluster.
     * Interpretaci√≥n autom√°tica en texto para cada cluster, explicando sus caracter√≠sticas de manera comprensible para un usuario com√∫n.

---

## M√©tricas utilizadas

* **Silhouette Score**: Indica qu√© tan separados y compactos est√°n los clusters.

  * Valor cercano a 1: clusters bien definidos.
  * Valor cercano a 0: clusters solapados o poco definidos.
  * Valor negativo: posibles errores en la asignaci√≥n de clusters.

* **Estad√≠sticas por cluster**:

  * Cantidad de lecturas por cluster.
  * Promedio de `Valor`, `Hora` y `DiaSemana_num` por cluster.

---

## Interpretaci√≥n

Cada cluster representa un **patr√≥n de comportamiento de lecturas**. Por ejemplo:

* Cluster 0: lecturas con valores altos en horas pico.
* Cluster 1: lecturas bajas en d√≠as espec√≠ficos de la semana.
* Cluster 2: lecturas promedio distribuidas de manera uniforme.

> Nota: La interpretaci√≥n se genera autom√°ticamente en el PDF para facilitar su comprensi√≥n por cualquier usuario.

---

## Archivos generados

* Modelo entrenado: `datos/modelos/kmeans_model.pkl`
* Informe PDF: `datos/informe_clusters.pdf`

---

## Uso en FastAPI

Existen endpoints para entrenar el modelo y descargar el PDF:

1. **Entrenar modelo y generar PDF**

```
GET /ml_unsupervised/entrenar/?n_clusters=3
```

2. **Descargar PDF**

```
GET /ml_unsupervised/descargar-informe/
```

---

## Bibliotecas utilizadas

* `pandas`
* `matplotlib`
* `sklearn` (`KMeans`, `StandardScaler`, `silhouette_score`)
* `joblib`
* `reportlab`

**Proyecto:** SmartWasteApi

